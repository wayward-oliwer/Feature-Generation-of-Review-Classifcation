{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOIDmsub9vWf"
   },
   "source": [
    "Import all the packages needed for the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50gwGuXt3vuY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import scipy as sp\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M15DI3f92MR"
   },
   "source": [
    "Initialisation of the reviews into positive and negative (break data into test, evaluation and training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQtPpfXY78PD"
   },
   "outputs": [],
   "source": [
    "pos = np.array([])\n",
    "neg = np.array([])\n",
    "\n",
    "files = os.listdir (\"data/pos\")\n",
    "for file in files:\n",
    "    f = open(\"data/pos/\" + file, 'r')\n",
    "    data = f.read().replace(\"<br />\", ' ')\n",
    "    f.close()\n",
    "    pos = np.append(pos, data.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "\n",
    "files = os.listdir (\"data/neg\")\n",
    "for file in files:\n",
    "    f = open(\"data/neg/\" + file, 'r')\n",
    "    data = f.read().replace(\"<br />\", ' ')\n",
    "    f.close()\n",
    "    neg = np.append(neg, data.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "#10% for evaluation, 20% for testing, and 70% for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDfGIeh599hA"
   },
   "source": [
    "3 methods of tokenisation and the different arrays that they form (split by whitespace, lemmatized, stemmed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVu3PdpI9upL"
   },
   "outputs": [],
   "source": [
    "# Splitting by whitespace of the positive and negative reviews\n",
    "white_space_pos = np.char.split(pos)\n",
    "white_space_neg = np.char.split(neg)\n",
    "\n",
    "\n",
    "# Lemmatizisation of the positive and negative reviews\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_pos = np.char.split(pos)\n",
    "lemmatized_neg = np.char.split(neg)\n",
    "\n",
    "for i in range(len(white_space_pos)):\n",
    "    for j in range(len(white_space_pos[i])):\n",
    "        lemmatized_pos[i][j] = lemmatizer.lemmatize(lemmatized_pos[i][j],pos=\"v\")\n",
    "        lemmatized_pos[i][j] = lemmatizer.lemmatize(lemmatized_pos[i][j],pos=\"n\")\n",
    "        lemmatized_pos[i][j] = lemmatizer.lemmatize(lemmatized_pos[i][j],pos=\"a\")\n",
    "        lemmatized_pos[i][j] = lemmatizer.lemmatize(lemmatized_pos[i][j],pos=\"r\")\n",
    "        lemmatized_pos[i][j] = lemmatizer.lemmatize(lemmatized_pos[i][j],pos=\"s\")\n",
    "\n",
    "for i in range(len(white_space_neg)):\n",
    "    for j in range(len(white_space_neg[i])):\n",
    "        lemmatized_neg[i][j] = lemmatizer.lemmatize(lemmatized_neg[i][j],pos=\"v\")\n",
    "        lemmatized_neg[i][j] = lemmatizer.lemmatize(lemmatized_neg[i][j],pos=\"n\")\n",
    "        lemmatized_neg[i][j] = lemmatizer.lemmatize(lemmatized_neg[i][j],pos=\"a\")\n",
    "        lemmatized_neg[i][j] = lemmatizer.lemmatize(lemmatized_neg[i][j],pos=\"r\")\n",
    "        lemmatized_neg[i][j] = lemmatizer.lemmatize(lemmatized_neg[i][j],pos=\"s\")\n",
    "\n",
    "\n",
    "# Stemming of the positive and negative reviews\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_pos = np.char.split(pos)\n",
    "stemmed_neg = np.char.split(neg)\n",
    "for i in range(len(white_space_pos)):\n",
    "    for j in range(len(white_space_pos[i])):\n",
    "        stemmed_pos[i][j] = stemmer.stem(stemmed_pos[i][j])\n",
    "        \n",
    "for i in range(len(white_space_neg)):\n",
    "    for j in range(len(white_space_neg[i])):\n",
    "        stemmed_neg[i][j] = stemmer.stem(stemmed_neg[i][j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAUzOtbCq6-l"
   },
   "source": [
    "Finding the words that are very common within the reviews in order to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4IOMPJzq6-l"
   },
   "outputs": [],
   "source": [
    "# Creates a frequency table of all the words within the positive and negative reviews\n",
    "words = np.array([])\n",
    "freq = np.array([])\n",
    "def find(x,y):\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == y:\n",
    "            return (True, i)\n",
    "    return (False,0)\n",
    "\n",
    "# Goes through all the reviews word by word and add it to the words array if it isn't \n",
    "# in it already or increases its frequency if it is.\n",
    "for i in range(len(stemmed_pos)):\n",
    "    for j in range(len(stemmed_pos[i])):\n",
    "        found,x = find(words,stemmed_pos[i][j])\n",
    "\n",
    "        if not found:\n",
    "            words = np.append(words,stemmed_pos[i][j])\n",
    "            freq = np.append(freq,1)\n",
    "\n",
    "        else:\n",
    "            freq[x] += 1\n",
    "\n",
    "for i in range(len(stemmed_neg)):\n",
    "    for j in range(len(stemmed_neg[i])):\n",
    "        found,x = find(words,stemmed_neg[i][j])\n",
    "\n",
    "        if not found:\n",
    "            words = np.append(words,stemmed_neg[i][j])\n",
    "            freq = np.append(freq,1)\n",
    "\n",
    "        else:\n",
    "            freq[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O1LBTRBq6-l"
   },
   "outputs": [],
   "source": [
    "# Finds the words that are very common (above 10000 occurences) and records it as a \n",
    "# word to delete\n",
    "todelete = np.where(freq > 10000)[0]\n",
    "nodelete = np.delete(words,todelete)\n",
    "nd = np.char.str_len(nodelete)\n",
    "delete = np.where(nd<= 2)[0]\n",
    "nodelete = np.delete(nodelete,delete)\n",
    "\n",
    "\n",
    "# Goes through the array of review features and checks if that feature/word should be \n",
    "# deleted due to it being too common.\n",
    "#\n",
    "# As all features are the same length at this point, the stemmed features were used to\n",
    "# find the length of all the feature arrays.\n",
    "for i in range(len(stemmed_neg)):\n",
    "    lengthi = len(stemmed_neg[i])\n",
    "    j = 0\n",
    "\n",
    "    while (j < lengthi):\n",
    "        found,x = find(nodelete,stemmed_neg[i][j])\n",
    "\n",
    "        if not found:\n",
    "            stemmed_neg[i] = np.delete(stemmed_neg[i],j)\n",
    "            lemmatized_neg[i] = np.delete(lemmatized_neg[i],j)\n",
    "            white_space_neg[i] = np.delete(white_space_neg[i],j)\n",
    "            lengthi-=1\n",
    "\n",
    "        else:\n",
    "            j+=1\n",
    "\n",
    "for i in range(len(stemmed_pos)):\n",
    "    lengthi = len(stemmed_pos[i])\n",
    "    j = 0\n",
    "\n",
    "    while (j<lengthi):\n",
    "        found,x = find(nodelete,stemmed_pos[i][j])\n",
    "\n",
    "        if not found:\n",
    "            stemmed_pos[i] = np.delete(stemmed_pos[i],j)\n",
    "            lemmatized_pos[i] = np.delete(lemmatized_pos[i],j)\n",
    "            white_space_pos[i] = np.delete(white_space_pos[i],j)\n",
    "            lengthi -= 1\n",
    "\n",
    "        else:\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR8c1YAAq6-m"
   },
   "source": [
    "As the searching of common words and their deletion took a long time, progress was saved by putting the reviews without the common phrases into new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4ejnXaAq6-m"
   },
   "outputs": [],
   "source": [
    "pos = np.array([])\n",
    "neg = np.array([])\n",
    "\n",
    "for i in range(2000):\n",
    "    f = open(\"data/\" + str(i) + \"p.txt\", 'r')\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    pos = np.append(pos, data)\n",
    "\n",
    "for i in range(1997):\n",
    "    f = open(\"data/\" + str(i) + \"n.txt\", 'r')\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    neg = np.append(neg, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAkT_lEP-FDJ"
   },
   "source": [
    "Code for Compositional Phrases:\\\n",
    "One part is for finding n-grams of the words. Those n-grams got stored as the process took relatively long to compute. The second part is PoS and constituency parsing for noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO0LXIiL-F6-"
   },
   "outputs": [],
   "source": [
    "# Finds the n-grams when n = 2\n",
    "#\n",
    "# Goes through a sample of 500 of the positive and negative reviews to find common n-grams.\n",
    "# 500 was picked as the sample size as it was a large enough sample that could represent that dataset\n",
    "# well, while not taking an long time to run.\n",
    "#\n",
    "# Please note that the variables of this code were changed between stemmed, lemmatized and white spaces\n",
    "# to acquire the different n-grams for each tokenisation method. The code below features the code for the\n",
    "# lemmatizisation method.\n",
    "words = []\n",
    "freq = []\n",
    "for i in range(500):\n",
    "    for j in range(len(lemmatized_pos[i])-(1)):\n",
    "\n",
    "        if (lemmatized_pos[i][j]+\"-\"+lemmatized_pos[i][j+(1)]) not in words:\n",
    "            words = np.append(words,(lemmatized_pos[i][j]+\"-\"+lemmatized_pos[i][j+(1)]))\n",
    "            freq = np.append(freq,1)\n",
    "            words = words.tolist()\n",
    "            freq = freq.tolist()\n",
    "\n",
    "        else:\n",
    "            words = np.array(words)\n",
    "            x = np.where(words == (lemmatized_pos[i][j]+\"-\"+lemmatized_pos[i][j+(1)]))[0][0]\n",
    "            freq[x] += 1\n",
    "            words = words.tolist()\n",
    "\n",
    "for i in range(500):\n",
    "    for j in range(len(lemmatized_neg[i])-(1)):\n",
    "\n",
    "        if (lemmatized_neg[i][j]+\"-\"+lemmatized_neg[i][j+(1)]) not in words:\n",
    "            words = np.append(words,(lemmatized_neg[i][j]+\"-\"+lemmatized_neg[i][j+(1)]))\n",
    "            freq = np.append(freq,1)\n",
    "            words = words.tolist()\n",
    "            freq = freq.tolist()\n",
    "\n",
    "        else:\n",
    "            words = np.array(words)\n",
    "            x = np.where(words == (lemmatized_neg[i][j]+\"-\"+lemmatized_neg[i][j+(1)]))[0][0]\n",
    "            freq[x] += 1\n",
    "            words = words.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQ08dUbRq6-n"
   },
   "outputs": [],
   "source": [
    "# The code mirrors that of the code above, but simple for n-grams where n = 3.\n",
    "words_3 = []\n",
    "freq_3 = []\n",
    "for i in range(500):\n",
    "    for j in range(len(lemmatized_pos[i])-2):\n",
    "\n",
    "        if (lemmatized_pos[i][j]+\"-\"+lemmatized_pos[i][j+(1)]+\"-\"+lemmatized_pos[i][j+(2)]) not in words_3:\n",
    "            words_3 = np.append(words_3,(lemmatized_pos[i][j]+\"-\"+lemmatized_pos[i][j+(1)]+\"-\"+lemmatized_pos[i][j+(2)]))\n",
    "            freq_3 = np.append(freq_3,1)\n",
    "            words_3 = words_3.tolist()\n",
    "            freq_3 = freq_3.tolist()\n",
    "        \n",
    "        else:\n",
    "            words_3 = np.array(words_3)\n",
    "            x = np.where(words_3 == (lemmatized_pos[i][j]+\"-\"+lemmatized_pos[i][j+(1)]+\"-\"+lemmatized_pos[i][j+(2)]))[0][0]\n",
    "            freq_3[x] += 1\n",
    "            words_3 = words_3.tolist()\n",
    "\n",
    "for i in range(500):\n",
    "    for j in range(len(lemmatized_neg[i])-(2)):\n",
    "\n",
    "        if (lemmatized_neg[i][j]+\"-\"+lemmatized_neg[i][j+(1)]+\"-\"+lemmatized_neg[i][j+(2)]) not in words_3:\n",
    "            words_3 = np.append(words_3,(lemmatized_neg[i][j]+\"-\"+lemmatized_neg[i][j+(1)]+\"-\"+lemmatized_neg[i][j+(2)]))\n",
    "            freq_3 = np.append(freq_3,1)\n",
    "            words_3 = words_3.tolist()\n",
    "            freq_3 = freq_3.tolist()\n",
    "        \n",
    "        else:\n",
    "            words_3 = np.array(words_3)\n",
    "            x = np.where(words_3 == (lemmatized_neg[i][j]+\"-\"+lemmatized_neg[i][j+(1)]+\"-\"+lemmatized_neg[i][j+(2)]))[0][0]\n",
    "            freq_3[x] += 1\n",
    "            words_3 = words_3.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the n-gram search process was quite time consuming to run, the features were stored in text files for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9Ial1m-q6-n"
   },
   "outputs": [],
   "source": [
    "s3w = \" \".join(str(x) for x in words_3)\n",
    "s3f = \" \".join(str(x) for x in freq_3)\n",
    "f = open(\"data/3ngraml.txt\",\"w\")\n",
    "f.write(s3w+\"\\n\")\n",
    "f.write(s3f+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvgHDqZDq6-n"
   },
   "outputs": [],
   "source": [
    "s2w = \" \".join(str(x) for x in words)\n",
    "s2f = \" \".join(str(x) for x in freq)\n",
    "f = open(\"data/ngramwl.txt\",\"w\")\n",
    "f.write(s2w+\"\\n\")\n",
    "f.write(s2f+\"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was run after all n-grams of each of the tokenisation method were found as it read the findings from their assigned files. This was done to prevent time being lost rerunning the previous code sections in case of a crash or other technical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PqSukh8q6-o"
   },
   "outputs": [],
   "source": [
    "f = open(\"data/ngrams.txt\",\"r\")\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "s2w = np.array(data.split())\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "s2fw = data.split()\n",
    "\n",
    "freq2s = np.array([])\n",
    "for i in range(len(s2fw)):\n",
    "    freq2s = np.append(freq2s, float(s2fw[i]))\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/3ngrams.txt\",\"r\")\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "s3w = np.array(data.split())\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "s3fw = data.split()\n",
    "\n",
    "freq3s = np.array([])\n",
    "for i in range(len(s3fw)):\n",
    "    freq3s = np.append(freq3s, float(s3fw[i]))\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ngramwl.txt\",\"r\")\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "l2w = np.array(data.split())\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "l2fw = data.split()\n",
    "\n",
    "freq2l = np.array([])\n",
    "for i in range(len(l2fw)):\n",
    "    freq2l = np.append(freq2l, float(l2fw[i]))\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/3ngraml.txt\",\"r\")\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "l3w = np.array(data.split())\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "l3fw = data.split()\n",
    "\n",
    "freq3l = np.array([])\n",
    "for i in range(len(l3fw)):\n",
    "    freq3l = np.append(freq3l, float(l3fw[i]))\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ngramws.txt\",\"r\")\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "ws2w = np.array(data.split())\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "ws2fw = data.split()\n",
    "\n",
    "freq2ws = np.array([])\n",
    "for i in range(len(ws2fw)):\n",
    "    freq2ws = np.append(freq2ws, float(ws2fw[i]))\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/3ngramws.txt\",\"r\")\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "ws3w = np.array(data.split())\n",
    "data = f.readline()\n",
    "data = data.replace(\"\\n\", '')\n",
    "ws3fw = data.split()\n",
    "\n",
    "freq3ws = np.array([])\n",
    "for i in range(len(ws3fw)):\n",
    "    freq3ws = np.append(freq3ws, float(ws3fw[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K68_bD2Jq6-o"
   },
   "outputs": [],
   "source": [
    "todelete = np.where(freq2ws < 30)[0]\n",
    "ws_nwords = np.delete(ws2w,todelete)\n",
    "ws_2words = ws_nwords.tolist()\n",
    "ws_poswords = ['special-effects','New-York','low-budget']\n",
    "todelete = np.where(freq3ws < 30)[0]\n",
    "ws_3words = np.delete(ws3w,todelete)\n",
    "ws_3words = ws_3words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJCvjQatq6-o"
   },
   "outputs": [],
   "source": [
    "todelete = np.where(freq2l < 30)[0]\n",
    "lem_nwords = np.delete(l2w,todelete)\n",
    "lem_2words = lem_nwords.tolist()\n",
    "lem_poswords = ['special-effect','horror-film','main-character','horror-movie','New-York','bad-movie', 'good-movie']\n",
    "todelete = np.where(freq3l < 30)[0]\n",
    "lem_3words = np.delete(l3w,todelete)\n",
    "lem_2words = lem_3words.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mMZdcI6q6-o"
   },
   "outputs": [],
   "source": [
    "todelete = np.where(freq2s < 30)[0]\n",
    "stem_nwords = np.delete(s2w,todelete)\n",
    "stem_2words = stem_nwords.tolist()\n",
    "stem_poswords = ['special-effect','horror-film','main-charact','horror-movi', 'low-budget', 'new-york']\n",
    "todelete = np.where(freq3s < 30)[0]\n",
    "stem_3words = np.delete(s3w,todelete)\n",
    "stem_3words = stem_3words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZExZ27QOq6-p"
   },
   "outputs": [],
   "source": [
    "ws_ngramsp = np.copy(white_space_pos)\n",
    "ws_ngramsn = np.copy(white_space_neg)\n",
    "\n",
    "lem_ngramsp = np.copy(lemmatized_pos)\n",
    "lem_ngramsn = np.copy(lemmatized_neg)\n",
    "\n",
    "stem_ngramsp = np.copy(stemmed_pos)\n",
    "stem_ngramsn = np.copy(stemmed_neg)\n",
    "\n",
    "\n",
    "#Stemmed\n",
    "for i in range(len(stemmed_pos)):\n",
    "    lengthi = len(stemmed_pos[i])-2\n",
    "    j = 0\n",
    "\n",
    "    while (j<lengthi):\n",
    "        found_3 = stem_ngramsp[i][j]+\"-\"+stem_ngramsp[i][j+1]+\"-\"+stem_ngramsp[i][j+2] in stem_3words\n",
    "        found_2 = stem_ngramsp[i][j]+\"-\"+stem_ngramsp[i][j+1] in stem_2words\n",
    "\n",
    "        if found_3:\n",
    "            stem_ngramsp[i][j] = stem_ngramsp[i][j]+\"-\"+stem_ngramsp[i][j+1]+\"-\"+stem_ngramsp[i][j+2]\n",
    "            stem_ngramsp[i] = np.delete(stem_ngramsp[i],j+1)\n",
    "            stem_ngramsp[i] = np.delete(stem_ngramsp[i],j+1)\n",
    "            lengthi -= 2\n",
    "\n",
    "        elif found_2:\n",
    "            stem_ngramsp[i][j] = stem_ngramsp[i][j]+\"-\"+stem_ngramsp[i][j+1]\n",
    "            stem_ngramsp[i] = np.delete(stem_ngramsp[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    if (j<(len(stem_ngramsp[i])-1)):\n",
    "        found_2 = stem_ngramsp[i][j]+\"-\"+stem_ngramsp[i][j+1] in stem_2words\n",
    "\n",
    "        if found_2:\n",
    "            stem_ngramsp[i][j] = stem_ngramsp[i][j]+\"-\"+stem_ngramsp[i][j+1]\n",
    "            stem_ngramsp[i] = np.delete(stem_ngramsp[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "for i in range(len(stemmed_neg)):\n",
    "    lengthi = len(stemmed_neg[i])-2\n",
    "    j = 0\n",
    "\n",
    "    while (j<lengthi):\n",
    "        found_3 = stem_ngramsn[i][j]+\"-\"+stem_ngramsn[i][j+1]+\"-\"+stem_ngramsn[i][j+2] in stem_3words\n",
    "        found_2 = stem_ngramsn[i][j]+\"-\"+stem_ngramsn[i][j+1] in stem_2words\n",
    "\n",
    "        if found_3:\n",
    "            stem_ngramsn[i][j] = stem_ngramsn[i][j]+\"-\"+stem_ngramsn[i][j+1]+\"-\"+stem_ngramsn[i][j+2]\n",
    "            stem_ngramsn[i] = np.delete(stem_ngramsn[i],j+1)\n",
    "            stem_ngramsn[i] = np.delete(stem_ngramsn[i],j+1)\n",
    "            lengthi -= 2\n",
    "\n",
    "        elif found_2:\n",
    "            stem_ngramsn[i][j] = stem_ngramsn[i][j]+\"-\"+stem_ngramsn[i][j+1]\n",
    "            stem_ngramsn[i] = np.delete(stem_ngramsn[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    if (j<(len(stem_ngramsn[i])-1)):\n",
    "        found_2 = stem_ngramsn[i][j]+\"-\"+stem_ngramsn[i][j+1] in stem_2words\n",
    "\n",
    "        if found_2:\n",
    "            stem_ngramsn[i][j] = stem_ngramsn[i][j]+\"-\"+stem_ngramsn[i][j+1]\n",
    "            stem_ngramsn[i] = np.delete(stem_ngramsn[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "\n",
    "#Lemantized\n",
    "for i in range(len(lemmatized_pos)):\n",
    "    lengthi = len(lemmatized_pos[i])-2\n",
    "    j = 0\n",
    "\n",
    "    while (j<lengthi):\n",
    "        found_3 = lem_ngramsp[i][j]+\"-\"+lem_ngramsp[i][j+1]+\"-\"+lem_ngramsp[i][j+2] in lem_3words\n",
    "        found_2 = lem_ngramsp[i][j]+\"-\"+lem_ngramsp[i][j+1] in lem_2words\n",
    "\n",
    "        if found_3:\n",
    "            lem_ngramsp[i][j] = lem_ngramsp[i][j]+\"-\"+lem_ngramsp[i][j+1]+\"-\"+lem_ngramsp[i][j+2]\n",
    "            lem_ngramsp[i] = np.delete(lem_ngramsp[i],j+1)\n",
    "            lem_ngramsp[i] = np.delete(lem_ngramsp[i],j+1)\n",
    "            lengthi -= 2\n",
    "\n",
    "        elif found_2:\n",
    "            lem_ngramsp[i][j] = lem_ngramsp[i][j]+\"-\"+lem_ngramsp[i][j+1]\n",
    "            lem_ngramsp[i] = np.delete(lem_ngramsp[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    if (j<(len(lem_ngramsp[i])-1)):\n",
    "        found_2 = lem_ngramsp[i][j]+\"-\"+lem_ngramsp[i][j+1] in lem_2words\n",
    "\n",
    "        if found_2:\n",
    "            lem_ngramsp[i][j] = lem_ngramsp[i][j]+\"-\"+lem_ngramsp[i][j+1]\n",
    "            lem_ngramsp[i] = np.delete(lem_ngramsp[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "for i in range(len(lemmatized_neg)):\n",
    "    lengthi = len(lemmatized_neg[i])-2\n",
    "    j = 0\n",
    "\n",
    "    while (j<lengthi):\n",
    "        found_3 = lem_ngramsn[i][j]+\"-\"+lem_ngramsn[i][j+1]+\"-\"+lem_ngramsn[i][j+2] in lem_3words\n",
    "        found_2 = lem_ngramsn[i][j]+\"-\"+lem_ngramsn[i][j+1] in lem_2words\n",
    "\n",
    "        if found_3:\n",
    "            lem_ngramsn[i][j] = lem_ngramsn[i][j]+\"-\"+lem_ngramsn[i][j+1]+\"-\"+lem_ngramsn[i][j+2]\n",
    "            lem_ngramsn[i] = np.delete(lem_ngramsn[i],j+1)\n",
    "            lem_ngramsn[i] = np.delete(lem_ngramsn[i],j+1)\n",
    "            lengthi -= 2\n",
    "\n",
    "        elif found_2:\n",
    "            lem_ngramsn[i][j] = lem_ngramsn[i][j]+\"-\"+lem_ngramsn[i][j+1]\n",
    "            lem_ngramsn[i] = np.delete(lem_ngramsn[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    if (j<(len(lem_ngramsn[i])-1)):\n",
    "        found_2 = lem_ngramsn[i][j]+\"-\"+lem_ngramsn[i][j+1] in lem_2words\n",
    "\n",
    "        if found_2:\n",
    "            lem_ngramsn[i][j] = lem_ngramsn[i][j]+\"-\"+lem_ngramsn[i][j+1]\n",
    "            lem_ngramsn[i] = np.delete(lem_ngramsn[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "\n",
    "# White Space\n",
    "for i in range(len(white_space_pos)):\n",
    "    lengthi = len(white_space_pos[i])-2\n",
    "    j = 0\n",
    "\n",
    "    while (j<lengthi):\n",
    "        found_3 = ws_ngramsp[i][j]+\"-\"+ws_ngramsp[i][j+1]+\"-\"+ws_ngramsp[i][j+2] in ws_3words\n",
    "        found_2 = ws_ngramsp[i][j]+\"-\"+ws_ngramsp[i][j+1] in ws_2words\n",
    "\n",
    "        if found_3:\n",
    "            ws_ngramsp[i][j] = ws_ngramsp[i][j]+\"-\"+ws_ngramsp[i][j+1]+\"-\"+ws_ngramsp[i][j+2]\n",
    "            ws_ngramsp[i] = np.delete(ws_ngramsp[i],j+1)\n",
    "            ws_ngramsp[i] = np.delete(ws_ngramsp[i],j+1)\n",
    "            lengthi -= 2\n",
    "\n",
    "        elif found_2:\n",
    "            ws_ngramsp[i][j] = ws_ngramsp[i][j]+\"-\"+ws_ngramsp[i][j+1]\n",
    "            ws_ngramsp[i] = np.delete(ws_ngramsp[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    if (j<(len(ws_ngramsp[i])-1)):\n",
    "        found_2 = ws_ngramsp[i][j]+\"-\"+ws_ngramsp[i][j+1] in ws_2words\n",
    "\n",
    "        if found_2:\n",
    "            ws_ngramsp[i][j] = ws_ngramsp[i][j]+\"-\"+ws_ngramsp[i][j+1]\n",
    "            ws_ngramsp[i] = np.delete(ws_ngramsp[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "for i in range(len(white_space_neg)):\n",
    "    lengthi = len(white_space_neg[i])-2\n",
    "    j = 0\n",
    "\n",
    "    while (j<lengthi):\n",
    "        found_3 = ws_ngramsn[i][j]+\"-\"+ws_ngramsn[i][j+1]+\"-\"+ws_ngramsn[i][j+2] in ws_3words\n",
    "        found_2 = ws_ngramsn[i][j]+\"-\"+ws_ngramsn[i][j+1] in ws_2words\n",
    "\n",
    "        if found_3:\n",
    "            ws_ngramsn[i][j] = ws_ngramsn[i][j]+\"-\"+ws_ngramsn[i][j+1]+\"-\"+ws_ngramsn[i][j+2]\n",
    "            ws_ngramsn[i] = np.delete(ws_ngramsn[i],j+1)\n",
    "            ws_ngramsn[i] = np.delete(ws_ngramsn[i],j+1)\n",
    "            lengthi -= 2\n",
    "\n",
    "        elif found_2:\n",
    "            ws_ngramsn[i][j] = ws_ngramsn[i][j]+\"-\"+ws_ngramsn[i][j+1]\n",
    "            ws_ngramsn[i] = np.delete(ws_ngramsn[i],j+1)\n",
    "            lengthi -= 1\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    if (j<(len(ws_ngramsn[i])-1)):\n",
    "        found_2 = ws_ngramsn[i][j]+\"-\"+ws_ngramsn[i][j+1] in ws_2words\n",
    "        \n",
    "        if found_2:\n",
    "            ws_ngramsn[i][j] = ws_ngramsn[i][j]+\"-\"+ws_ngramsn[i][j+1]\n",
    "            ws_ngramsn[i] = np.delete(ws_ngramsn[i],j+1)\n",
    "            lengthi -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxl8yR-oq6-p"
   },
   "outputs": [],
   "source": [
    "stem_posp = np.copy(stemmed_pos)\n",
    "stem_posn = np.copy(stemmed_neg)\n",
    "lem_posp = np.copy(lemmatized_pos)\n",
    "lem_posn = np.copy(lemmatized_neg)\n",
    "ws_posp = np.copy(white_space_pos)\n",
    "ws_posn = np.copy(white_space_neg)\n",
    "\n",
    "#Stemmed\n",
    "for i in range(len(stemmed_pos)):\n",
    "    lengthi = len(stemmed_pos[i])-1\n",
    "    j = 0\n",
    "    while (j<lengthi):\n",
    "        found = stem_posp[i][j]+\"-\"+stem_posp[i][j+1] in stem_poswords\n",
    "        if found:\n",
    "            stem_posp[i][j] = stem_posp[i][j]+\"-\"+stem_posp[i][j+1]\n",
    "            stem_posp[i] = np.delete(stem_posp[i],j+1)\n",
    "            lengthi -= 1\n",
    "        j += 1\n",
    "\n",
    "for i in range(len(stemmed_neg)):\n",
    "    lengthi = len(stemmed_neg[i])-1\n",
    "    j = 0\n",
    "    while (j<lengthi):\n",
    "        found = stem_posn[i][j]+\"-\"+stem_posn[i][j+1] in stem_poswords\n",
    "        if found:\n",
    "            stem_posn[i][j] = stem_posn[i][j]+\"-\"+stem_posn[i][j+1]\n",
    "            stem_posn[i] = np.delete(stem_posn[i],j+1)\n",
    "            lengthi -= 1\n",
    "        j += 1\n",
    "#Lemmatized\n",
    "for i in range(len(lemmatized_pos)):\n",
    "    lengthi = len(lemmatized_pos[i])-1\n",
    "    j = 0\n",
    "    while (j<lengthi):\n",
    "        found = lem_posp[i][j]+\"-\"+lem_posp[i][j+1] in lem_poswords\n",
    "        if found:\n",
    "            lem_posp[i][j] = lem_posp[i][j]+\"-\"+lem_posp[i][j+1]\n",
    "            lem_posp[i] = np.delete(lem_posp[i],j+1)\n",
    "            lengthi -= 1\n",
    "        j += 1\n",
    "\n",
    "for i in range(len(lemmatized_neg)):\n",
    "    lengthi = len(lemmatized_neg[i])-1\n",
    "    j = 0\n",
    "    while (j<lengthi):\n",
    "        found = lem_posn[i][j]+\"-\"+lem_posn[i][j+1] in lem_poswords\n",
    "        if found:\n",
    "            lem_posn[i][j] = lem_posn[i][j]+\"-\"+lem_posn[i][j+1]\n",
    "            lem_posn[i] = np.delete(lem_posn[i],j+1)\n",
    "            lengthi -= 1\n",
    "        j += 1\n",
    "\n",
    "#White Space\n",
    "for i in range(len(white_space_pos)):\n",
    "    lengthi = len(white_space_pos[i])-1\n",
    "    j = 0\n",
    "    while (j<lengthi):\n",
    "        found = ws_posp[i][j]+\"-\"+ws_posp[i][j+1] in ws_poswords\n",
    "        if found:\n",
    "            ws_posp[i][j] = ws_posp[i][j]+\"-\"+ws_posp[i][j+1]\n",
    "            ws_posp[i] = np.delete(ws_posp[i],j+1)\n",
    "            lengthi -= 1\n",
    "        j += 1\n",
    "\n",
    "for i in range(len(white_space_neg)):\n",
    "    lengthi = len(white_space_neg[i])-1\n",
    "    j = 0\n",
    "    while (j<lengthi):\n",
    "        found = ws_posn[i][j]+\"-\"+ws_posn[i][j+1] in ws_poswords\n",
    "        if found:\n",
    "            ws_posn[i][j] = ws_posn[i][j]+\"-\"+ws_posn[i][j+1]\n",
    "            ws_posn[i] = np.delete(ws_posn[i],j+1)\n",
    "            lengthi -= 1\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Bonus Feature: \\\n",
    "This code splits abbreciated words into two separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNYFeksSq6-q"
   },
   "outputs": [],
   "source": [
    "feature_set = np.append(lem_ngramsp,lem_ngramsn)\n",
    "for i in range(len(feature_set)): #Goes through each review in the set\n",
    "    lengthi = len(feature_set[i])\n",
    "    j = 0\n",
    "\n",
    "    while (j<lengthi): #Goes through each word in the review\n",
    "        \n",
    "        #First section uses words that can't simply be split by getting rid of the \"nt\"\n",
    "        if feature_set[i][j].lower() == \"wont\":\n",
    "            feature_set[i][j] = \"would\"\n",
    "            feature_set[i] = np.insert(feature_set[i], j+1,\"not\")\n",
    "            j+=2\n",
    "            lengthi += 1\n",
    "\n",
    "        elif feature_set[i][j].lower() == \"cant\":\n",
    "            feature_set[i][j] = \"can\"\n",
    "            feature_set[i] = np.insert(feature_set[i], j+1,\"not\")\n",
    "            j+=2\n",
    "            lengthi += 1\n",
    "\n",
    "\n",
    "        #Second section get rid of the ending of the word and adds it as a separate word\n",
    "        elif feature_set[i][j].lower() == \"wouldnt\":\n",
    "            feature_set[i][j] = \"would\" #Removes the \"nt\" and stores that word is its current location\n",
    "            \n",
    "            feature_set[i] = np.insert(feature_set[i], j+1,\"not\") #Inserts the \"not\" into the next location\n",
    "            \n",
    "            j+=2 #j is incremented by 2 to skip the new \"not\" element that got added\n",
    "            \n",
    "            lengthi += 1 #The length of the review is incremented by 1\n",
    "\n",
    "        elif feature_set[i][j].lower() == \"couldnt\":\n",
    "            feature_set[i][j] = \"could\"\n",
    "            feature_set[i] = np.insert(feature_set[i], j+1,\"not\")\n",
    "            j+=2\n",
    "            lengthi += 1\n",
    "\n",
    "        elif feature_set[i][j].lower() == \"youre\":\n",
    "            feature_set[i][j] = \"you\"\n",
    "            feature_set[i] = np.insert(feature_set[i], j+1,\"are\")\n",
    "            j+=2\n",
    "            lengthi += 1\n",
    "\n",
    "        elif feature_set[i][j].lower() == \"youll\":\n",
    "            feature_set[i][j] = \"you\"\n",
    "            feature_set[i] = np.insert(feature_set[i], j+1,\"will\")\n",
    "            j+=2\n",
    "            lengthi += 1\n",
    "\n",
    "        elif feature_set[i][j].lower() == \"youve\":\n",
    "            feature_set[i][j] = \"you\"\n",
    "            feature_set[i] = np.insert(feature_set[i], j+1,\"have\")\n",
    "            j+=2\n",
    "            lengthi += 1\n",
    "\n",
    "        elif feature_set[i][j].lower() == \"ive\":\n",
    "            feature_set[i][j] = \"I\"\n",
    "            feature_set[i] = np.insert(feature_set[i], j+1,\"have\")\n",
    "            j+=2\n",
    "            lengthi += 1\n",
    "\n",
    "        #The same happens for the remaining elif statements but for different word endings\n",
    "\n",
    "        #If none of the statements ar true, go to the next word in the review\n",
    "        else:\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O587Rbrnq6-p"
   },
   "outputs": [],
   "source": [
    "#The implementation of PoS using algorithm was attempted. However, after hours of difficulties with fixing NLTKs bugs with the pos_tag algorithm,\n",
    "# a manual method of going through n-grams to see which nouns are together was used instead. Below is the code that would have been run if no\n",
    "# issues with pos_tag would have occurred.\n",
    "'''\n",
    "words_pos = []\n",
    "freq_pos = []\n",
    "phrases_pos = []\n",
    "regexes = 'CHUNK: {<NOUN> <NOUN>}'\n",
    "for i in range(500):\n",
    "    tokens = nltk.word_tokenize(stemmed_pos[i])\n",
    "    tag = nltk.pos_tag(tokens, tagset='universal')\n",
    "    tree = noun_phrase_regex.parse(tag)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'CHUNK':\n",
    "            if subtree in words_pos:\n",
    "                words_pos = np.array(words_pos)\n",
    "                x = np.where(words_pos == subtree)[0][0]\n",
    "                freq_pos[x] += 1\n",
    "                words_pos = words_pos.tolist()\n",
    "            else:\n",
    "                words_pos.append(subtree)\n",
    "                freq_pos = np.append(freq_pos,1)\n",
    "                freq_pos = freq_pos.tolist()\n",
    "print(\"Positive explored...\")\n",
    "for i in range(500):\n",
    "    tokens = nltk.word_tokenize(stemmed_neg[i])\n",
    "    tag = nltk.pos_tag(tokens, tagset='universal')\n",
    "    tree = noun_phrase_regex.parse(tag)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'CHUNK':\n",
    "            if subtree in words_pos:\n",
    "                words_pos = np.array(words_pos)\n",
    "                x = np.where(words_pos == subtree)[0][0]\n",
    "                freq_pos[x] += 1\n",
    "                words_pos = words_pos.tolist()\n",
    "            else:\n",
    "                words_pos.append(subtree)\n",
    "                freq_pos = np.append(freq_pos,1)\n",
    "                freq_pos = freq_pos.tolist()\n",
    "for wordy in words_pos:\n",
    "    leaves = wordy.leaves()\n",
    "    phrases_pos.append(' '.join([word for word, _ in leaves]))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRHaZpXr-de2"
   },
   "source": [
    "Code that puts the data is a format that can then be split into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GBnj_Ejq6-q"
   },
   "outputs": [],
   "source": [
    "stemposp = np.array([])\n",
    "stemposn = np.array([])\n",
    "lemposp = np.array([])\n",
    "lemposn = np.array([])\n",
    "wsposp = np.array([])\n",
    "wsposn = np.array([])\n",
    "stemngramsp = np.array([])\n",
    "stemngramsn = np.array([])\n",
    "lemngramsp = np.array([])\n",
    "lemngramsn = np.array([])\n",
    "wsngramsp = np.array([])\n",
    "wsngramsn = np.array([])\n",
    "\n",
    "for i in range(2000):\n",
    "    stemposp =np.append(stemposp, \" \".join(str(x) for x in stem_posp[i]))\n",
    "    stemngramsp =np.append(stemngramsp, \" \".join(str(x) for x in stem_ngramsp[i]))\n",
    "    lemposp =np.append(lemposp, \" \".join(str(x) for x in lem_posp[i]))\n",
    "    lemngramsp =np.append(lemngramsp, \" \".join(str(x) for x in lem_ngramsp[i]))\n",
    "    wsposp =np.append(wsposp, \" \".join(str(x) for x in ws_posp[i]))\n",
    "    wsngramsp =np.append(wsngramsp, \" \".join(str(x) for x in ws_ngramsp[i]))\n",
    "for i in range(1996):\n",
    "    stemposn =np.append(stemposn, \" \".join(str(x) for x in stem_posn[i]))\n",
    "    stemngramsn =np.append(stemngramsn, \" \".join(str(x) for x in stem_ngramsn[i]))\n",
    "    lemposn =np.append(lemposn, \" \".join(str(x) for x in lem_posn[i]))\n",
    "    lemngramsn =np.append(lemngramsn, \" \".join(str(x) for x in lem_ngramsn[i]))\n",
    "    wsposn =np.append(wsposn, \" \".join(str(x) for x in ws_posn[i]))\n",
    "    wsngramsn =np.append(wsngramsn, \" \".join(str(x) for x in ws_ngramsn[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLW5s5zbq6-q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels = np.append(np.ones(2000),np.zeros(1997))\n",
    "stem_pos_data = np.append(stemposp,stemposn)\n",
    "lem_pos_data = np.append(lemposp,lemposn)\n",
    "ws_pos_data = np.append(wsposp,wsposn)\n",
    "\n",
    "stem_ngrams_data = np.append(stemngramsp,stemngramsn)\n",
    "lem_ngrams_data = np.append(lemngramsp,lemngramsn)\n",
    "ws_ngrams_data = np.append(wsngramsp,wsngramsn)\n",
    "\n",
    "data = np.append(pos,neg)\n",
    "\n",
    "data_train,data_test,label_train, label_test = train_test_split(data, labels, test_size=0.30, random_state=24)\n",
    "\n",
    "stem_pos_data_train,stem_pos_data_test,stem_pos_label_train, stem_pos_label_test = train_test_split(stem_pos_data, labels, test_size=0.30, random_state=24)\n",
    "lem_pos_data_train,lem_pos_data_test,lem_pos_label_train, lem_pos_label_test = train_test_split(lem_pos_data, labels, test_size=0.30, random_state=24)\n",
    "ws_pos_data_train,ws_pos_data_test,ws_pos_label_train, ws_pos_label_test = train_test_split(ws_pos_data, labels, test_size=0.30, random_state=24)\n",
    "\n",
    "stem_ngrams_data_train,stem_ngrams_data_test,stem_ngrams_label_train, stem_ngrams_label_test = train_test_split(stem_ngrams_data, labels, test_size=0.30, random_state=24)\n",
    "lem_ngrams_data_train,lem_ngrams_data_test,lem_ngrams_label_train, lem_ngrams_label_test = train_test_split(lem_ngrams_data, labels, test_size=0.30, random_state=24)\n",
    "ws_ngrams_data_train,ws_ngrams_data_test,ws_ngrams_label_train, ws_ngrams_label_test = train_test_split(ws_ngrams_data, labels, test_size=0.30, random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el2Qh4LEtimW"
   },
   "source": [
    "Code for Normalisation section:\\\n",
    "These are the two normalisation methods that were made from scratch. The first one being TF-IDF and the second being TF-RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTcJR4dnq6-r"
   },
   "outputs": [],
   "source": [
    "class tfidfvectoriser:\n",
    "    dictionary = np.array([])\n",
    "\n",
    "    def fit_transform(self, array):\n",
    "        self.create_dict(np.char.split(array)) #Fitting part of the fit_transform\n",
    "        terms = self.transform(array) #Transforming part of the fit_transform\n",
    "        return terms\n",
    "\n",
    "    def transform(self,array):\n",
    "        array = np.char.split(array)\n",
    "        terms = np.zeros((len(array),len(self.dictionary))) #Creates the array of tokens that initially starts with a frequency of 0 for all\n",
    "        terms = terms.tolist()\n",
    "\n",
    "        for i in range(len(array)): #Goes through all the reviews passed into the transform function\n",
    "            n = self.count_words(array[i]) #Creates a frequency table for a review based on the dictionary\n",
    "            sum_d = sum(n)\n",
    "            n = n/sum_d #Acquires the TF value of the tokens\n",
    "            terms[i] = n\n",
    "\n",
    "        idf = np.zeros(len(self.dictionary))\n",
    "\n",
    "        for i in range(len(self.dictionary)):\n",
    "            for j in range(len(array)):\n",
    "              if terms[j][i] != 0.0: #Sees if the term is in the file (non-zero)\n",
    "                idf[i]+= 1 #If it is in it, increments count by 1\n",
    "\n",
    "        idf = (1+len(array))/(1+idf)\n",
    "        idf = np.log(idf) #Acquires the IDF value of terms\n",
    "\n",
    "        terms = terms * idf #Acquires the TF-IDF values\n",
    "        return terms\n",
    "\n",
    "\n",
    "    def count_words(self,array):\n",
    "        freq = np.zeros(len(self.dictionary))\n",
    "        for i in range(len(array)):\n",
    "            if array[i].lower() in self.dictionary: #Sees if the lowercase version of the word is in the dictionary\n",
    "                self.dictionary = np.array(self.dictionary)\n",
    "                x = np.where(self.dictionary == array[i].lower())[0][0]\n",
    "                freq[x] += 1 #If so, it increments the freq of that word appearing in this review\n",
    "                self.dictionary = self.dictionary.tolist()\n",
    "        \n",
    "        return freq\n",
    "\n",
    "    def create_dict(self,array):\n",
    "        for i in range(len(array)): #Goes through all reviews\n",
    "            for j in range(len(array[i])): #Goes through all words in the review\n",
    "                if array[i][j].lower() not in self.dictionary: #Sees if the lowercase version of the word is in the dictionary\n",
    "                    self.dictionary = np.append(self.dictionary,array[i][j].lower()) #If not, it adds the lowercase version of the word to the dictionary\n",
    "                    self.dictionary = self.dictionary.tolist()\n",
    "\n",
    "class tfrfvectoriser():\n",
    "    dictionary = np.array([])\n",
    "    rf = np.array([])\n",
    "    def fit_transform(self, array,lables):\n",
    "        self.create_dict(np.char.split(array)) #Fitting part of the fit_transform\n",
    "        self.create_rf(array,lables) #Acquires RF values of terms based on labels\n",
    "        terms = self.transform(array) #Transforming part of the fit_transform\n",
    "        return terms\n",
    "\n",
    "    def transform(self,array):\n",
    "        array = np.char.split(array)\n",
    "        terms = np.zeros((len(array),len(self.dictionary))) #Creates the array of tokens that initially starts with a frequency of 0 for all\n",
    "        terms = terms.tolist()\n",
    "        for i in range(len(array)): #Goes through all the reviews passed into the transform function\n",
    "            n = self.count_words(array[i])\n",
    "            sum_d = sum(n)\n",
    "            tf = n/sum_d #Acquires the TF value of the tokens\n",
    "            tfrf = tf*self.rf #Acquires the TF-RF value of the tokens\n",
    "            terms[i] = tfrf\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def count_words(self,array):\n",
    "        freq = np.zeros(len(self.dictionary))\n",
    "        for i in range(len(array)):\n",
    "            if array[i].lower() in self.dictionary:\n",
    "                self.dictionary = np.array(self.dictionary)\n",
    "                x = np.where(self.dictionary == array[i].lower())[0][0]\n",
    "                freq[x] += 1\n",
    "                self.dictionary = self.dictionary.tolist()\n",
    "        return freq\n",
    "\n",
    "    def create_dict(self,array):\n",
    "        for i in range(len(array)):\n",
    "            for j in range(len(array[i])):\n",
    "                if array[i][j].lower() not in self.dictionary:\n",
    "                    self.dictionary = np.append(self.dictionary,array[i][j].lower())\n",
    "                    self.dictionary = self.dictionary.tolist()\n",
    "\n",
    "\n",
    "    def create_rf(self,array,lables):\n",
    "        self.rf = np.zeros(len(self.dictionary)) #Creates the array of rf values for each of the words in the dictionary\n",
    "        for i in range(len(self.dictionary)): #Goes through each term in the dictionary\n",
    "            a = 0 #Number of positive reviews with the term\n",
    "            c = 0 #Number of negative reviews without the term\n",
    "            \n",
    "            for j in range(len(array)):\n",
    "                if self.dictionary[i] in array[j].lower():\n",
    "                    if lables[j] == 1: #If the term is in the review and its a positive review, a is incremented\n",
    "                        a += 1\n",
    "                \n",
    "                else:\n",
    "                    if lables[j] == 0: #If the term isn't in the review and its a negative review, c is incremented\n",
    "                        c += 1\n",
    "            \n",
    "            if 1>c: #The max function in the formula\n",
    "                c = 1\n",
    "            \n",
    "            self.rf[i] = np.log(2 + (a/c)) #Acquires the RF value for the term in the dictionary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGOPSQ_L-nST"
   },
   "source": [
    "Code for Feature Selection: \\\n",
    "Runs all possible combinations on the evaluation data and computes the accuracy, precision, and recall of each feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OAtZUVW-myW"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "tfvectorizer = tfidfvectoriser()\n",
    "stem_ngrams_data_train_tfidf = tfvectorizer.fit_transform(stem_ngrams_data_train)\n",
    "stem_ngrams_data_test_tfidf = tfvectorizer.transform(stem_ngrams_data_test[0:int(len(data_test)*0.33)])\n",
    "stem_ngrams_tf_mnb = MultinomialNB()\n",
    "stem_ngrams_tf_mnb.fit(stem_ngrams_data_train_tfidf,stem_ngrams_label_train)\n",
    "stem_ngrams_tf_y = stem_ngrams_tf_mnb.predict(stem_ngrams_data_test_tfidf)\n",
    "print(\"Stem N-grams TF-IDF\")\n",
    "print(\"Accuracy\", accuracy_score(stem_ngrams_label_test[0:int(len(data_test)*0.33)], stem_ngrams_tf_y))\n",
    "print(\"Precision\", precision_score(stem_ngrams_label_test[0:int(len(data_test)*0.33)], stem_ngrams_tf_y))\n",
    "print(\"Recall\", recall_score(stem_ngrams_label_test[0:int(len(data_test)*0.33)], stem_ngrams_tf_y))\n",
    "print(\"\")\n",
    "\n",
    "tfvectorizer = tfidfvectoriser()\n",
    "lem_ngrams_data_train_tfidf = tfvectorizer.fit_transform(lem_ngrams_data_train)\n",
    "lem_ngrams_data_test_tfidf = tfvectorizer.transform(lem_ngrams_data_test[0:int(len(data_test)*0.33)])\n",
    "lem_ngrams_tf_mnb = MultinomialNB()\n",
    "lem_ngrams_tf_mnb.fit(lem_ngrams_data_train_tfidf,lem_ngrams_label_train)\n",
    "lem_ngrams_tf_y = lem_ngrams_tf_mnb.predict(lem_ngrams_data_test_tfidf)\n",
    "print(\"Lem N-grams TF-IDF\")\n",
    "print(\"Accuracy\", accuracy_score(lem_ngrams_label_test[0:int(len(data_test)*0.33)], lem_ngrams_tf_y))\n",
    "print(\"Precision\", precision_score(lem_ngrams_label_test[0:int(len(data_test)*0.33)], lem_ngrams_tf_y))\n",
    "print(\"Recall\", recall_score(lem_ngrams_label_test[0:int(len(data_test)*0.33)], lem_ngrams_tf_y))\n",
    "print(\"\")\n",
    "\n",
    "tfvectorizer = tfidfvectoriser()\n",
    "ws_ngrams_data_train_tfidf = tfvectorizer.fit_transform(ws_ngrams_data_train)\n",
    "ws_ngrams_data_test_tfidf = tfvectorizer.transform(ws_ngrams_data_test[0:int(len(data_test)*0.33)])\n",
    "ws_ngrams_tf_mnb = MultinomialNB()\n",
    "ws_ngrams_tf_mnb.fit(ws_ngrams_data_train_tfidf,ws_ngrams_label_train)\n",
    "ws_ngrams_tf_y = ws_ngrams_tf_mnb.predict(ws_ngrams_data_test_tfidf)\n",
    "print(\"White Space N-grams TF-IDF\")\n",
    "print(\"Accuracy\", accuracy_score(ws_ngrams_label_test[0:int(len(data_test)*0.33)], ws_ngrams_tf_y))\n",
    "print(\"Precision\", precision_score(ws_ngrams_label_test[0:int(len(data_test)*0.33)], ws_ngrams_tf_y))\n",
    "print(\"Recall\", recall_score(ws_ngrams_label_test[0:int(len(data_test)*0.33)], ws_ngrams_tf_y))\n",
    "print(\"\")\n",
    "\n",
    "tfvectorizer = tfidfvectoriser()\n",
    "stem_pos_data_train_tfidf = tfvectorizer.fit_transform(stem_pos_data_train)\n",
    "stem_pos_data_test_tfidf = tfvectorizer.transform(stem_pos_data_test[0:int(len(data_test)*0.33)])\n",
    "stem_pos_tf_mnb = MultinomialNB()\n",
    "stem_pos_tf_mnb.fit(stem_pos_data_train_tfidf,stem_pos_label_train)\n",
    "stem_pos_tf_y = stem_pos_tf_mnb.predict(stem_pos_data_test_tfidf)\n",
    "print(\"Stem PoS TF-IDF\")\n",
    "print(\"Accuracy\", accuracy_score(stem_pos_label_test[0:int(len(data_test)*0.33)], stem_pos_tf_y))\n",
    "print(\"Precision\", precision_score(stem_pos_label_test[0:int(len(data_test)*0.33)], stem_pos_tf_y))\n",
    "print(\"Recall\", recall_score(stem_pos_label_test[0:int(len(data_test)*0.33)], stem_pos_tf_y))\n",
    "print(\"\")\n",
    "\n",
    "tfvectorizer = tfidfvectoriser()\n",
    "lem_pos_data_train_tfidf = tfvectorizer.fit_transform(lem_pos_data_train)\n",
    "lem_pos_data_test_tfidf = tfvectorizer.transform(lem_pos_data_test[0:int(len(data_test)*0.33)])\n",
    "lem_pos_tf_mnb = MultinomialNB()\n",
    "lem_pos_tf_mnb.fit(lem_pos_data_train_tfidf,lem_pos_label_train)\n",
    "lem_pos_tf_y = lem_pos_tf_mnb.predict(lem_pos_data_test_tfidf)\n",
    "print(\"Lem PoS TF-IDF\")\n",
    "print(\"Accuracy\", accuracy_score(lem_pos_label_test[0:int(len(data_test)*0.33)], lem_pos_tf_y))\n",
    "print(\"Precision\", precision_score(lem_pos_label_test[0:int(len(data_test)*0.33)], lem_pos_tf_y))\n",
    "print(\"Recall\", recall_score(lem_pos_label_test[0:int(len(data_test)*0.33)], lem_pos_tf_y))\n",
    "print(\"\")\n",
    "\n",
    "tfvectorizer = tfidfvectoriser()\n",
    "ws_pos_data_train_tfidf = tfvectorizer.fit_transform(ws_pos_data_train)\n",
    "ws_pos_data_test_tfidf = tfvectorizer.transform(ws_pos_data_test[0:int(len(data_test)*0.33)])\n",
    "ws_pos_tf_mnb = MultinomialNB()\n",
    "ws_pos_tf_mnb.fit(ws_pos_data_train_tfidf,ws_pos_label_train)\n",
    "ws_pos_tf_y = ws_pos_tf_mnb.predict(ws_pos_data_test_tfidf)\n",
    "print(\"White Space PoS TF-IDF\")\n",
    "print(\"Accuracy\", accuracy_score(ws_pos_label_test[0:int(len(data_test)*0.33)], ws_pos_tf_y))\n",
    "print(\"Precision\", precision_score(ws_pos_label_test[0:int(len(data_test)*0.33)], ws_pos_tf_y))\n",
    "print(\"Recall\", recall_score(ws_pos_label_test[0:int(len(data_test)*0.33)], ws_pos_tf_y))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "cvectorizer = tfrfvectoriser()\n",
    "stem_ngrams_data_train_count = cvectorizer.fit_transform(stem_ngrams_data_train,stem_ngrams_label_train)\n",
    "stem_ngrams_data_test_count = cvectorizer.transform(stem_ngrams_data_test[0:int(len(data_test)*0.33)])\n",
    "stem_ngrams_count_mnb = MultinomialNB()\n",
    "stem_ngrams_count_mnb.fit(stem_ngrams_data_train_count,stem_ngrams_label_train)\n",
    "stem_ngrams_count_y = stem_ngrams_count_mnb.predict(stem_ngrams_data_test_count)\n",
    "print(\"Stem N-grams TF-RF\")\n",
    "print(\"Accuracy\", accuracy_score(stem_ngrams_label_test[0:int(len(data_test)*0.33)], stem_ngrams_count_y))\n",
    "print(\"Precision\", precision_score(stem_ngrams_label_test[0:int(len(data_test)*0.33)], stem_ngrams_count_y))\n",
    "print(\"Recall\", recall_score(stem_ngrams_label_test[0:int(len(data_test)*0.33)], stem_ngrams_count_y))\n",
    "print(\"\")\n",
    "\n",
    "cvectorizer = tfrfvectoriser()\n",
    "lem_ngrams_data_train_count = cvectorizer.fit_transform(lem_ngrams_data_train,lem_ngrams_label_train)\n",
    "lem_ngrams_data_test_count = cvectorizer.transform(lem_ngrams_data_test[0:int(len(data_test)*0.33)])\n",
    "lem_ngrams_count_mnb = MultinomialNB()\n",
    "lem_ngrams_count_mnb.fit(lem_ngrams_data_train_count,lem_ngrams_label_train)\n",
    "lem_ngrams_count_y = lem_ngrams_count_mnb.predict(lem_ngrams_data_test_count)\n",
    "print(\"Lem N-grams TF-RF\")\n",
    "print(\"Accuracy\", accuracy_score(lem_ngrams_label_test[0:int(len(data_test)*0.33)], lem_ngrams_count_y))\n",
    "print(\"Precision\", precision_score(lem_ngrams_label_test[0:int(len(data_test)*0.33)], lem_ngrams_count_y))\n",
    "print(\"Recall\", recall_score(lem_ngrams_label_test[0:int(len(data_test)*0.33)], lem_ngrams_count_y))\n",
    "print(\"\")\n",
    "\n",
    "cvectorizer = tfrfvectoriser()\n",
    "ws_ngrams_data_train_count = cvectorizer.fit_transform(ws_ngrams_data_train,ws_ngrams_label_train)\n",
    "ws_ngrams_data_test_count = cvectorizer.transform(ws_ngrams_data_test[0:int(len(data_test)*0.33)])\n",
    "ws_ngrams_count_mnb = MultinomialNB()\n",
    "ws_ngrams_count_mnb.fit(ws_ngrams_data_train_count,ws_ngrams_label_train)\n",
    "ws_ngrams_count_y = ws_ngrams_count_mnb.predict(ws_ngrams_data_test_count)\n",
    "print(\"White Space N-grams TF-RF\")\n",
    "print(\"Accuracy\", accuracy_score(ws_ngrams_label_test[0:int(len(data_test)*0.33)], ws_ngrams_count_y))\n",
    "print(\"Precision\", precision_score(ws_ngrams_label_test[0:int(len(data_test)*0.33)], ws_ngrams_count_y))\n",
    "print(\"Recall\", recall_score(ws_ngrams_label_test[0:int(len(data_test)*0.33)], ws_ngrams_count_y))\n",
    "print(\"\")\n",
    "\n",
    "cvectorizer = tfrfvectoriser()\n",
    "ws_pos_data_train_count = cvectorizer.fit_transform(ws_pos_data_train,ws_pos_label_train)\n",
    "ws_pos_data_test_count = cvectorizer.transform(ws_pos_data_test[0:int(len(data_test)*0.33)])\n",
    "ws_pos_count_mnb = MultinomialNB()\n",
    "ws_pos_count_mnb.fit(ws_pos_data_train_count,ws_pos_label_train)\n",
    "ws_pos_count_y = ws_pos_count_mnb.predict(ws_pos_data_test_count)\n",
    "print(\"White Space PoS TF-RF\")\n",
    "print(\"Accuracy\", accuracy_score(ws_pos_label_test[0:int(len(data_test)*0.33)], ws_pos_count_y))\n",
    "print(\"Precision\", precision_score(ws_pos_label_test[0:int(len(data_test)*0.33)], ws_pos_count_y))\n",
    "print(\"Recall\", recall_score(ws_pos_label_test[0:int(len(data_test)*0.33)], ws_pos_count_y))\n",
    "print(\"\")\n",
    "\n",
    "cvectorizer = tfrfvectoriser()\n",
    "stem_pos_data_train_count = cvectorizer.fit_transform(stem_pos_data_train,stem_pos_label_train)\n",
    "stem_pos_data_test_count = cvectorizer.transform(stem_pos_data_test[0:int(len(data_test)*0.33)])\n",
    "stem_pos_count_mnb = MultinomialNB()\n",
    "stem_pos_count_mnb.fit(stem_pos_data_train_count,stem_pos_label_train)\n",
    "stem_pos_count_y = stem_pos_count_mnb.predict(stem_pos_data_test_count)\n",
    "print(\"Stem PoS TF-RF\")\n",
    "print(\"Accuracy\", accuracy_score(stem_pos_label_test[0:int(len(data_test)*0.33)], stem_pos_count_y))\n",
    "print(\"Precision\", precision_score(stem_pos_label_test[0:int(len(data_test)*0.33)], stem_pos_count_y))\n",
    "print(\"Recall\", recall_score(stem_pos_label_test[0:int(len(data_test)*0.33)], stem_pos_count_y))\n",
    "print(\"\")\n",
    "\n",
    "cvectorizer = tfrfvectoriser()\n",
    "lem_pos_data_train_count = cvectorizer.fit_transform(lem_pos_data_train,lem_pos_label_train)\n",
    "lem_pos_data_test_count = cvectorizer.transform(lem_pos_data_test[0:int(len(data_test)*0.33)])\n",
    "lem_pos_count_mnb = MultinomialNB()\n",
    "lem_pos_count_mnb.fit(lem_pos_data_train_count,lem_pos_label_train)\n",
    "lem_pos_count_y = lem_pos_count_mnb.predict(lem_pos_data_test_count)\n",
    "print(\"Lem PoS TF-RF\")\n",
    "print(\"Accuracy\", accuracy_score(lem_pos_label_test[0:int(len(data_test)*0.33)], lem_pos_count_y))\n",
    "print(\"Precision\", precision_score(lem_pos_label_test[0:int(len(data_test)*0.33)], lem_pos_count_y))\n",
    "print(\"Recall\", recall_score(lem_pos_label_test[0:int(len(data_test)*0.33)], lem_pos_count_y))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dHzm4GT-3u8"
   },
   "source": [
    "Code for Naive Bayes: \\\n",
    "This is the model that was made from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P-0ghdm-_ZM"
   },
   "outputs": [],
   "source": [
    "class naive_bayes:\n",
    "  def fit(self, data, labels):\n",
    "      #Likelihood of each feature effecting the label\n",
    "      self.neg_likelihood = np.zeros(len(data[0]))\n",
    "      self.pos_likelihood = np.zeros(len(data[0]))\n",
    "\n",
    "      #Total count of all words in type of review\n",
    "      self.pos_total_count = 0\n",
    "      self.neg_total_count = 0\n",
    "\n",
    "      #Total count of each word in type of review\n",
    "      self.pos_word_count = np.zeros(len(data[0]))\n",
    "      self.neg_word_count = np.zeros(len(data[0]))\n",
    "\n",
    "      #Counts and stores the word counts of positive and negative reviews\n",
    "      for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                if (labels[i] == 1):\n",
    "                    self.pos_total_count += 1\n",
    "                    self.pos_word_count[j] += data[i][j]\n",
    "                else:\n",
    "                    self.neg_total_count += 1\n",
    "                    self.neg_word_count[j] += data[i][j]\n",
    "\n",
    "      #Calculates average amount of words in positive and negative reviews\n",
    "      self.pos_av = self.pos_total_count / len(data)\n",
    "      self.neg_av = self.neg_total_count / len(data)\n",
    "\n",
    "      #Calculates the likelhoods of each word affecting the label\n",
    "      #+1 included for Laplace smoothing (avoids division by 0)\n",
    "      self.pos_likelihood = (self.pos_word_count + 1) / (np.sum(self.pos_word_count + 1))\n",
    "      self.neg_likelihood = (self.neg_word_count + 1) / (np.sum(self.neg_word_count + 1))\n",
    "\n",
    "\n",
    "  def predict(self, data):\n",
    "      labels = np.zeros(len(data))\n",
    "      for i in range(len(data)):\n",
    "\n",
    "          #Calculates the probability that the data is positive or negative\n",
    "          pos = np.sum((np.log(self.pos_likelihood) * data[i])) + np.log(self.pos_av)\n",
    "          neg = np.sum((np.log(self.neg_likelihood) * data[i])) + np.log(self.neg_av)\n",
    "\n",
    "          #If the probability for positive is bigger, its labelled 1\n",
    "          if pos > neg:\n",
    "              labels[i] = 1\n",
    "\n",
    "          #Otherwise, its labelled 0\n",
    "          else:\n",
    "            labels[i] = 0\n",
    "      \n",
    "      return labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3serR_UtPun"
   },
   "source": [
    "Code for Naive Bayes:\\\n",
    "This runs the different models on the test data of the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rKTDIBytPX1"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "tfvectorizer = tfidfvectoriser()\n",
    "\n",
    "lem_ngrams_data_train_tfidf = tfvectorizer.fit_transform(lem_ngrams_data_train)\n",
    "lem_ngrams_data_test_tfidf = tfvectorizer.transform(lem_ngrams_data_test[int(len(data_test)*0.33):])\n",
    "lem_ngrams_tf_mnb = MultinomialNB()\n",
    "lem_ngrams_tf_mnb.fit(lem_ngrams_data_train_tfidf,lem_ngrams_label_train)\n",
    "lem_ngrams_tf_y = lem_ngrams_tf_mnb.predict(lem_ngrams_data_test_tfidf)\n",
    "print(\"Sklearn\")\n",
    "print(\"Accuracy\", accuracy_score(lem_ngrams_label_test[int(len(data_test)*0.33):], lem_ngrams_tf_y))\n",
    "print(\"Precision\", precision_score(lem_ngrams_label_test[int(len(data_test)*0.33):], lem_ngrams_tf_y))\n",
    "print(\"Recall\", recall_score(lem_ngrams_label_test[int(len(data_test)*0.33):], lem_ngrams_tf_y))\n",
    "print(\"\")\n",
    "\n",
    "em_ngrams_tf_mnbs = naive_bayes()\n",
    "lem_ngrams_tf_mnbs.fit(lem_ngrams_data_train_tfidf,lem_ngrams_label_train)\n",
    "lem_ngrams_tf_ys = lem_ngrams_tf_mnbs.predict(lem_ngrams_data_test_tfidf)\n",
    "print(\"Scratch\")\n",
    "print(\"Accuracy\", accuracy_score(lem_ngrams_label_test[int(len(data_test)*0.33):], lem_ngrams_tf_ys))\n",
    "print(\"Precision\", precision_score(lem_ngrams_label_test[int(len(data_test)*0.33):], lem_ngrams_tf_ys))\n",
    "print(\"Recall\", recall_score(lem_ngrams_label_test[int(len(data_test)*0.33):], lem_ngrams_tf_ys))\n",
    "print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
